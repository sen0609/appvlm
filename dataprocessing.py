from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import io
import tensorflow as tf
import json
from android_env.proto.a11y import android_accessibility_forest_pb2

# è¯»å–æ•°æ®é›†
raw_dataset = tf.data.TFRecordDataset("./dataset/android_control/android_control-00011-of-00020", compression_type="GZIP")
dataset_iterator = tf.compat.v1.data.make_one_shot_iterator(raw_dataset)

example = tf.train.Example.FromString(dataset_iterator.get_next().numpy())
features = example.features.feature

# å­—ä½“åŠ è½½ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
try:
    font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf"
    font = ImageFont.truetype(font_path, 20)
except IOError:
    font = None

# åˆ¤æ–­æ˜¯å¦ä¸ºå¯äº¤äº’æ§ä»¶
def is_interactive(node):
    if hasattr(node, "is_clickable") and node.is_clickable:
        return True
    if any(a.id in [1, 16] for a in node.actions):  # click or long_click
        return True
    return False

# æŸ¥æ‰¾åŒ…å«ç‚¹å‡»ç‚¹çš„ window
def find_target_window(windows, x, y):
    for window in windows:
        bounds = window.bounds_in_screen
        if bounds.left <= x < bounds.right and bounds.top <= y < bounds.bottom:
            return window
    return None

# è·å–è¯¥ window ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
def collect_nodes_from_window(window):
    return list(window.tree.nodes)

# åˆ¤æ–­ä¸€ä¸ªç‚¹æ˜¯å¦åœ¨ bounds ä¸­
def is_point_inside_bounds(x, y, bounds):
    return bounds.left <= x < bounds.right and bounds.top <= y < bounds.bottom

# æ‰¾å‡ºæ‰€æœ‰åŒ…å«ç‚¹å‡»ç‚¹çš„èŠ‚ç‚¹
def find_nodes_containing_point(nodes, x, y):
    return [node for node in nodes if is_point_inside_bounds(x, y, node.bounds_in_screen)]

# åˆ¤æ–­æ˜¯å¦å…·æœ‰è¯­ä¹‰ä¿¡æ¯
def has_semantic_info(node):
    return bool(
        getattr(node, "view_id_resource_name", "") or
        getattr(node, "content_description", "") or
        getattr(node, "text", "")
    )

def find_descendant_with_text(node, all_nodes):
    """
    æ‰¾å‡ºè¯¥èŠ‚ç‚¹çš„æ‰€æœ‰åä»£ä¸­æœ€â€œæœ‰è¯­ä¹‰â€çš„é‚£ä¸ªï¼ˆå« text / content_description / resource_idï¼‰ï¼Œ
    ä¼˜å…ˆ text > content_description > view_id_resource_name
    """
    best_node = None
    best_score = -1

    def dfs(n):
        nonlocal best_node, best_score
        score = 0
        if getattr(n, "text", ""):
            score += 3
        if getattr(n, "content_description", ""):
            score += 2
        if getattr(n, "view_id_resource_name", ""):
            score += 1
        if score > best_score:
            best_score = score
            best_node = n
        for cid in getattr(n, "child_ids", []):
            child = next((x for x in all_nodes if x.unique_id == cid), None)
            if child:
                dfs(child)

    dfs(node)
    return best_node if best_score > 0 else None


# æå–ç»“æ„åŒ–ä¿¡æ¯
def extract_node_info(node):
    return {
        "unique_id": node.unique_id,
        "class": node.class_name,
        "package": node.package_name,
        "text": getattr(node, "text", ""),
        "content description": getattr(node, "content_description", ""),
        "resource name": getattr(node, "view_id_resource_name", ""),
        "is_clickable": getattr(node, "is_clickable", False),
    }

# ä»ç‚¹å‡»åæ ‡æ„å»º click action
def convert_click_to_element_action(forest_bytes, x, y):
    forest = android_accessibility_forest_pb2.AndroidAccessibilityForest()
    forest.ParseFromString(forest_bytes)

    target_window = find_target_window(forest.windows, x, y)
    if target_window is None:
        #print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å«ç‚¹å‡»ç‚¹ ({x}, {y}) çš„çª—å£")
        return None

    all_nodes = collect_nodes_from_window(target_window)
    matched_nodes = find_nodes_containing_point(all_nodes, x, y)

    #print(f"\nğŸŸ¡ æ‰¾åˆ° {len(matched_nodes)} ä¸ªåŒ¹é…ç‚¹ ({x}, {y}) çš„èŠ‚ç‚¹")
    for node in matched_nodes:
        b = node.bounds_in_screen
        #print(f"  - ID={node.unique_id} Bounds=({b.left}, {b.top}, {b.right}, {b.bottom}) Class={node.class_name}")

    if not matched_nodes:
        return None

    # ä¼˜å…ˆé€‰æ‹©æœ€æ·±çš„èŠ‚ç‚¹ä½œä¸ºç›®æ ‡ï¼ˆæ¯”å¦‚ unique_id=43ï¼‰
    target_node = max(matched_nodes, key=lambda n: getattr(n, "depth", 0))

    # å°è¯•å‘ä¸‹æ‰¾æ›´å…·è¯­ä¹‰çš„å­èŠ‚ç‚¹ï¼ˆæ¯”å¦‚ unique_id=71ï¼‰
    refined_node = find_descendant_with_text(target_node, all_nodes)
    final_node = refined_node if refined_node else target_node

    #print(f"\nğŸ¯ æœ€ç»ˆé€‰ä¸­çš„èŠ‚ç‚¹ ID={final_node.unique_id}, Class={final_node.class_name}")
    return {
        "action type": "click",
        "target element": extract_node_info(final_node)
    }


# è·å–æˆªå›¾å’Œæ ‘
screenshots = features["screenshots"].bytes_list.value
trees_raw = features["accessibility_trees"].bytes_list.value
trees = [android_accessibility_forest_pb2.AndroidAccessibilityForest().FromString(t) for t in trees_raw]

# ç”¨æ¥å­˜å‚¨æ¯ä¸ªæˆªå›¾å’Œpromptçš„å¯¹åº”æ•°æ®
data = []

# éå†æ¯ä¸€å¸§ observation
for i, (screenshot_bytes, forest) in enumerate(zip(screenshots, trees)):
    image = Image.open(io.BytesIO(screenshot_bytes)).convert("RGB")
    draw = ImageDraw.Draw(image)

    # é€‰ä¸­å½“å‰æ¿€æ´»çª—å£
    active_window = next((w for w in forest.windows if w.is_active), forest.windows[-1])

    count = 0
    for node in active_window.tree.nodes:
        b = node.bounds_in_screen
        if b.left < b.right and b.top < b.bottom and is_interactive(node):
            # ç»˜åˆ¶çº¢è‰²è¾¹æ¡†
            draw.rectangle([(b.left, b.top), (b.right, b.bottom)], outline="red", width=2)

            label = str(count)
            if font:
                text_size = draw.textbbox((0, 0), label, font=font)
            else:
                text_size = draw.textbbox((0, 0), label)
            text_width = text_size[2] - text_size[0]
            text_height = text_size[3] - text_size[1]

            bg_rect = [(b.left, b.top), (b.left + text_width + 6, b.top + text_height + 4)]
            draw.rectangle(bg_rect, fill=(169, 169, 169, 128)) 

            draw.text((b.left + 3, b.top + 2), label, fill="white", font=font)
            count += 1

    # ä¿å­˜å›¾åƒåˆ°æ–‡ä»¶
    screenshot_path = f"output_images/screenshot_{i}.png"
    plt.figure(figsize=(6, 12))
    plt.imshow(image)
    plt.axis("off")
    plt.savefig(screenshot_path)

    # è·å–æ–‡æœ¬è¾“å…¥
    goal = features["goal"].bytes_list.value[0].decode("utf-8")
    actions = [json.loads(a.decode("utf-8")) for a in features["actions"].bytes_list.value]

    # åŠ¨æ€è·å–æœ€è¿‘çš„å‰5ä¸ª actions
    previous_actions = actions[max(0, i - 5):i]  # åªè·å–æœ€è¿‘çš„å‰5ä¸ªåŠ¨ä½œ

    # è·å–å½“å‰æˆªå›¾çš„ x, y åæ ‡
    action = actions[i] if i < len(actions) else None
    if action and "x" in action and "y" in action:
        x, y = action["x"], action["y"]
        # å¤„ç† click å’Œ long_press
        if action["action_type"] in ["click", "long_press"]:
            click_action = convert_click_to_element_action(trees_raw[i], x, y)
            if click_action:
                prompt = {
                    "screenshot": screenshot_path,
                    "Goal": goal,
                    "Previous Actions": previous_actions,  # ä»…å‚è€ƒæœ€è¿‘5æ¬¡çš„actions
                    "Label": action,  # å½“å‰æˆªå›¾çš„label
                    "Click Action": click_action  # å½“å‰ç‚¹å‡»çš„ç›®æ ‡UIå…ƒç´ ä¿¡æ¯
                }
                data.append(prompt)

# ä¿å­˜ä¸ºJSONæ–‡ä»¶
with open("output_images/prompts_with_click.json", "w") as json_file:
    json.dump(data, json_file, indent=4)

print("Data saved to 'output_images/prompts_with_click.json'")