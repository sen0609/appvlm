{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2fe024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs/anaconda3/envs/appvlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cs/anaconda3/envs/appvlm/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915ae2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"models/paligemma\", local_files_only=True).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"models/paligemma\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287f1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 假设你有一张图片（可以替换为你自己的图片路径）\n",
    "image = Image.open(\"./test0.png\")\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "# 处理图片和文本输入\n",
    "promopt = \"how many serial number in the picture?\"  # 替换为你自己的任务描述\n",
    "\n",
    "# 使用processor对图像和文本进行处理\n",
    "model_inputs = processor(text=promopt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d5a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # 检查是否有可用的GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "749d92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39ca3cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.53it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# 加载模型并确保使用GPU（如果可用）\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # 检查是否有可用的GPU\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"models/paligemma\", local_files_only=True).to(device).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"models/paligemma\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "882a6479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "# 假设你有一张图片（可以替换为你自己的图片路径）\n",
    "image = Image.open(\"./airpods.jpg\")\n",
    "image = image.convert(\"RGB\")\n",
    "#display(image)\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f90a024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[257152, 257152, 257152,  ...,   5642, 235265,    108]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[[[ 0.0039,  0.0039,  0.0039,  ...,  0.4039,  0.3961,  0.4118],\n",
       "          [ 0.0039,  0.0039,  0.0039,  ...,  0.3961,  0.3961,  0.4118],\n",
       "          [ 0.0039,  0.0039,  0.0039,  ...,  0.3961,  0.3961,  0.4118],\n",
       "          ...,\n",
       "          [-0.0196, -0.0196, -0.0118,  ...,  0.3333,  0.3412,  0.3412],\n",
       "          [-0.0196, -0.0196, -0.0118,  ...,  0.3333,  0.3255,  0.3255],\n",
       "          [-0.0196, -0.0196, -0.0118,  ...,  0.3412,  0.3098,  0.2941]],\n",
       "\n",
       "         [[ 0.0039,  0.0039,  0.0039,  ...,  0.3961,  0.3882,  0.4039],\n",
       "          [ 0.0039,  0.0039,  0.0039,  ...,  0.3882,  0.3882,  0.4039],\n",
       "          [ 0.0039,  0.0039,  0.0039,  ...,  0.3882,  0.3882,  0.4039],\n",
       "          ...,\n",
       "          [-0.0353, -0.0353, -0.0275,  ...,  0.3255,  0.3333,  0.3333],\n",
       "          [-0.0353, -0.0353, -0.0275,  ...,  0.3255,  0.3176,  0.3176],\n",
       "          [-0.0353, -0.0353, -0.0275,  ...,  0.3333,  0.3020,  0.2863]],\n",
       "\n",
       "         [[-0.0745, -0.0745, -0.0745,  ...,  0.3647,  0.3569,  0.3725],\n",
       "          [-0.0745, -0.0745, -0.0745,  ...,  0.3569,  0.3569,  0.3725],\n",
       "          [-0.0745, -0.0745, -0.0745,  ...,  0.3569,  0.3569,  0.3725],\n",
       "          ...,\n",
       "          [-0.1373, -0.1373, -0.1294,  ...,  0.2941,  0.3020,  0.3020],\n",
       "          [-0.1373, -0.1373, -0.1294,  ...,  0.2941,  0.2863,  0.2863],\n",
       "          [-0.1373, -0.1373, -0.1294,  ...,  0.3020,  0.2706,  0.2549]]]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 处理图片和文本输入\n",
    "prompt = \"<image>Describe the picture.\"  # 在文本开始处添加 <image> token\n",
    "model_inputs = processor(text= prompt,images=image, return_tensors=\"pt\").to(device)\n",
    "# 获取输入长度\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "\n",
    "\n",
    "decoded_text = processor.decode(model_inputs[\"input_ids\"][0].tolist())\n",
    "\n",
    "# 打印解码结果\n",
    "#print(decoded_text)\n",
    "\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7331a33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airpods\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 使用GPU进行推理\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc172841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs/anaconda3/envs/appvlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cs/anaconda3/envs/appvlm/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "/home/cs/anaconda3/envs/appvlm/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many serial number in the picture?\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "# 检查并选择设备（GPU 优先）\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # 检查是否有可用的GPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 加载模型并确保使用GPU（如果可用）\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\"models/paligemma\", local_files_only=True).to(device).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"models/paligemma\", local_files_only=True)\n",
    "\n",
    "# 处理图片和文本输入\n",
    "prompt = \"How many serial number in the picture?\"\n",
    "image = Image.open(\"./test0.png\")\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "# 使用 processor 对图像和文本进行处理，并确保所有张量都在相同的设备上\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 使用GPU进行推理\n",
    "with torch.no_grad():  # 使用 no_grad() 来避免计算梯度，节省内存\n",
    "    output = model.generate(**inputs, max_new_tokens=50, cache_implementation=\"static\")\n",
    "\n",
    "# 输出结果\n",
    "decoded_text = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
